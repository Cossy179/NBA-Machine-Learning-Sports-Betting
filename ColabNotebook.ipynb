{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cossy179/NBA-Machine-Learning-Sports-Betting/blob/master/ColabNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84FCh7pFCA2U"
      },
      "source": [
        "# ðŸ€ **Ultimate NBA Prediction System v3.0** ðŸ€\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ **The Most Advanced NBA Sports Betting AI**\n",
        "\n",
        "Welcome to the **Ultimate NBA Prediction System** - a state-of-the-art machine learning platform that combines multiple AI models, real-time data, and advanced analytics to deliver the most accurate NBA predictions available.\n",
        "\n",
        "### âœ¨ **What Makes This System Special?**\n",
        "\n",
        "ðŸŽ¯ **75%+ Accuracy** - Advanced ensemble models with proper validation  \n",
        "ðŸ¤– **AI-Powered Parlays** - Smart combination betting with correlation analysis  \n",
        "ðŸ“Š **Multi-Target Predictions** - Win/Loss, Spreads, Totals, Player Props  \n",
        "ðŸ”„ **Real-Time Data** - Live injuries, lineups, weather, travel data  \n",
        "ðŸ’° **Kelly Criterion** - Optimal bankroll management  \n",
        "ðŸ§ª **Backtested** - Validated on full 2023-24 NBA season  \n",
        "ðŸ“ˆ **Player Stats Integration** - Comprehensive NBA player database  \n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“‹ **Quick Start Guide**\n",
        "\n",
        "1. **Bootstrap** - Download and install all requirements\n",
        "2. **Train Models** - Build the AI prediction system (optional)\n",
        "3. **Get Predictions** - Run predictions for today's games\n",
        "4. **Backtest** - Validate performance on historical data\n",
        "\n",
        "---\n",
        "\n",
        "You can use this notebook on [Google Colab](https://colab.research.google.com/) with a **GPU Hardware Accelerator** for optimal performance!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBeeVTbb_WLL"
      },
      "source": [
        "# ðŸ› ï¸ **Step 1: Bootstrap System**\n",
        "\n",
        "This cell downloads the complete NBA prediction system and installs all required packages.\n",
        "\n",
        "**âš ï¸ Important:** Make sure you're using a **GPU runtime** for optimal performance!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "cEOhYxk-ASjw",
        "outputId": "b30146ae-21c1-4bad-e51b-d449f2a49988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ€ Initializing Ultimate NBA Prediction System v3.0...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¥ Downloading system files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "Cloning into 'NBA-Machine-Learning-Sports-Betting'...\n",
            "Updating files:  33% (38/113)\n",
            "Updating files:  34% (39/113)\n",
            "Updating files:  35% (40/113)\n",
            "Updating files:  36% (41/113)\n",
            "Updating files:  37% (42/113)\n",
            "Updating files:  38% (43/113)\n",
            "Updating files:  39% (45/113)\n",
            "Updating files:  40% (46/113)\n",
            "Updating files:  41% (47/113)\n",
            "Updating files:  42% (48/113)\n",
            "Updating files:  43% (49/113)\n",
            "Updating files:  44% (50/113)\n",
            "Updating files:  45% (51/113)\n",
            "Updating files:  46% (52/113)\n",
            "Updating files:  47% (54/113)\n",
            "Updating files:  48% (55/113)\n",
            "Updating files:  49% (56/113)\n",
            "Updating files:  50% (57/113)\n",
            "Updating files:  51% (58/113)\n",
            "Updating files:  52% (59/113)\n",
            "Updating files:  53% (60/113)\n",
            "Updating files:  54% (62/113)\n",
            "Updating files:  55% (63/113)\n",
            "Updating files:  56% (64/113)\n",
            "Updating files:  57% (65/113)\n",
            "Updating files:  58% (66/113)\n",
            "Updating files:  59% (67/113)\n",
            "Updating files:  60% (68/113)\n",
            "Updating files:  61% (69/113)\n",
            "Updating files:  62% (71/113)\n",
            "Updating files:  63% (72/113)\n",
            "Updating files:  64% (73/113)\n",
            "Updating files:  65% (74/113)\n",
            "Updating files:  66% (75/113)\n",
            "Updating files:  67% (76/113)\n",
            "Updating files:  68% (77/113)\n",
            "Updating files:  69% (78/113)\n",
            "Updating files:  70% (80/113)\n",
            "Updating files:  71% (81/113)\n",
            "Updating files:  72% (82/113)\n",
            "Updating files:  73% (83/113)\n",
            "Updating files:  74% (84/113)\n",
            "Updating files:  75% (85/113)\n",
            "Updating files:  76% (86/113)\n",
            "Updating files:  77% (88/113)\n",
            "Updating files:  78% (89/113)\n",
            "Updating files:  79% (90/113)\n",
            "Updating files:  80% (91/113)\n",
            "Updating files:  81% (92/113)\n",
            "Updating files:  82% (93/113)\n",
            "Updating files:  83% (94/113)\n",
            "Updating files:  84% (95/113)\n",
            "Updating files:  85% (97/113)\n",
            "Updating files:  86% (98/113)\n",
            "Updating files:  87% (99/113)\n",
            "Updating files:  88% (100/113)\n",
            "Updating files:  89% (101/113)\n",
            "Updating files:  90% (102/113)\n",
            "Updating files:  91% (103/113)\n",
            "Updating files:  92% (104/113)\n",
            "Updating files:  93% (106/113)\n",
            "Updating files:  94% (107/113)\n",
            "Updating files:  95% (108/113)\n",
            "Updating files:  96% (109/113)\n",
            "Updating files:  97% (110/113)\n",
            "Updating files:  98% (111/113)\n",
            "Updating files:  99% (112/113)\n",
            "Updating files: 100% (113/113)\n",
            "Updating files: 100% (113/113), done.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¦ Installing core packages...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'mv' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: colorama==0.4.6 in c:\\users\\alex\\documents\\code\\cossy179\\nba-machine-learning-sports-betting\\.venv\\lib\\site-packages (0.4.6)\n",
            "Collecting sbrscrape==0.0.10\n",
            "  Using cached sbrscrape-0.0.10-py3-none-any.whl.metadata (2.9 kB)\n",
            "Using cached sbrscrape-0.0.10-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: sbrscrape\n",
            "Successfully installed sbrscrape-0.0.10\n",
            "Collecting pandas==2.1.1\n",
            "  Using cached pandas-2.1.1.tar.gz (4.3 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  Ã— Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  â”‚ exit code: 2\n",
            "  â•°â”€> [106 lines of output]\n",
            "      + meson setup C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-install-r_fss5y_\\pandas_ba725ff0742849a39cdb1105567dddae C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-install-r_fss5y_\\pandas_ba725ff0742849a39cdb1105567dddae\\.mesonpy-1ult41nq\\build -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-install-r_fss5y_\\pandas_ba725ff0742849a39cdb1105567dddae\\.mesonpy-1ult41nq\\build\\meson-python-native-file.ini\n",
            "      The Meson build system\n",
            "      Version: 1.2.1\n",
            "      Source dir: C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-install-r_fss5y_\\pandas_ba725ff0742849a39cdb1105567dddae\n",
            "      Build dir: C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-install-r_fss5y_\\pandas_ba725ff0742849a39cdb1105567dddae\\.mesonpy-1ult41nq\\build\n",
            "      Build type: native build\n",
            "      Project name: pandas\n",
            "      Project version: 2.1.1\n",
            "      Activating VS 16.11.26\n",
            "      C compiler for the host machine: cl (msvc 19.29.30148 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.29.30148 for x64\")\n",
            "      C linker for the host machine: link link 14.29.30148.0\n",
            "      C++ compiler for the host machine: cl (msvc 19.29.30148 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.29.30148 for x64\")\n",
            "      C++ linker for the host machine: link link 14.29.30148.0\n",
            "      Cython compiler for the host machine: cython (cython 0.29.37)\n",
            "      Host machine cpu family: x86_64\n",
            "      Host machine cpu: x86_64\n",
            "      Program python found: YES (c:\\Users\\Alex\\Documents\\code\\Cossy179\\NBA-Machine-Learning-Sports-Betting\\.venv\\Scripts\\python.exe)\n",
            "      Run-time dependency python found: YES 3.13\n",
            "      Build targets in project: 53\n",
            "      \n",
            "      pandas 2.1.1\n",
            "      \n",
            "        User defined options\n",
            "          Native files: C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-install-r_fss5y_\\pandas_ba725ff0742849a39cdb1105567dddae\\.mesonpy-1ult41nq\\build\\meson-python-native-file.ini\n",
            "          buildtype   : release\n",
            "          vsenv       : True\n",
            "          b_ndebug    : if-release\n",
            "          b_vscrt     : md\n",
            "      \n",
            "      Found ninja.EXE-1.13.0.git.kitware.jobserver-pipe-1 at C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-trsqc7t6\\normal\\Scripts\\ninja.EXE\n",
            "      \n",
            "      Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:\n",
            "      C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-trsqc7t6\\overlay\\Scripts\\meson compile -C .\n",
            "      + meson compile\n",
            "      Activating VS 16.11.26\n",
            "      INFO: automatically activated MSVC compiler environment\n",
            "      INFO: autodetecting backend as ninja\n",
            "      INFO: calculating backend command to run: C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-trsqc7t6\\normal\\Scripts\\ninja.EXE\n",
            "      [1/151] Generating pandas/_libs/algos_take_helper_pxi with a custom command\n",
            "      [2/151] Generating pandas/_libs/khash_primitive_helper_pxi with a custom command\n",
            "      [3/151] Generating pandas/_libs/intervaltree_helper_pxi with a custom command\n",
            "      [4/151] Generating pandas/_libs/hashtable_func_helper_pxi with a custom command\n",
            "      [5/151] Generating pandas/_libs/algos_common_helper_pxi with a custom command\n",
            "      [6/151] Generating pandas/_libs/hashtable_class_helper_pxi with a custom command\n",
            "      [7/151] Generating pandas/_libs/sparse_op_helper_pxi with a custom command\n",
            "      [8/151] Generating pandas/_libs/index_class_helper_pxi with a custom command\n",
            "      [9/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/base.pyx\n",
            "      [10/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/dtypes.pyx\n",
            "      [11/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/ccalendar.pyx\n",
            "      [12/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/np_datetime.pyx\n",
            "      [13/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/nattype.pyx\n",
            "      [14/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/conversion.pyx\n",
            "      [15/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/fields.pyx\n",
            "      [16/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/offsets.pyx\n",
            "      [17/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/parsing.pyx\n",
            "      [18/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/strptime.pyx\n",
            "      [19/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/timestamps.pyx\n",
            "      [20/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/period.pyx\n",
            "      [21/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/timezones.pyx\n",
            "      [22/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/timedeltas.pyx\n",
            "      [23/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/tzconversion.pyx\n",
            "      [24/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/indexing.pyx\n",
            "      [25/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/arrays.pyx\n",
            "      [26/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslibs/vectorized.pyx\n",
            "      [27/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/hashing.pyx\n",
            "      [28/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/index.pyx\n",
            "      [29/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/internals.pyx\n",
            "      [30/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/hashtable.pyx\n",
            "      [31/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/groupby.pyx\n",
            "      [32/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/interval.pyx\n",
            "      [33/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/missing.pyx\n",
            "      [34/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/algos.pyx\n",
            "      [35/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/lib.pyx\n",
            "      [36/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/ops_dispatch.pyx\n",
            "      [37/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/join.pyx\n",
            "      [38/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/parsers.pyx\n",
            "      [39/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/properties.pyx\n",
            "      [40/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/byteswap.pyx\n",
            "      [41/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/ops.pyx\n",
            "      [42/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/sas.pyx\n",
            "      [43/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/reshape.pyx\n",
            "      [44/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/testing.pyx\n",
            "      [45/151] Compiling C object pandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.obj\n",
            "      \u001b[31mFAILED: [code=2] \u001b[0mpandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.obj\n",
            "      \"cl\" \"-Ipandas\\_libs\\tslibs\\ccalendar.cp313-win_amd64.pyd.p\" \"-Ipandas\\_libs\\tslibs\" \"-I..\\..\\pandas\\_libs\\tslibs\" \"-I..\\..\\..\\..\\pip-build-env-trsqc7t6\\overlay\\Lib\\site-packages\\numpy\\_core\\include\" \"-I..\\..\\pandas\\_libs\\include\" \"-IC:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python313\\Include\" \"-DNDEBUG\" \"/MD\" \"/nologo\" \"/showIncludes\" \"/utf-8\" \"-w\" \"/O2\" \"/Gw\" \"-DNPY_NO_DEPRECATED_API=0\" \"-DNPY_TARGET_VERSION=NPY_1_21_API_VERSION\" \"/Fdpandas\\_libs\\tslibs\\ccalendar.cp313-win_amd64.pyd.p\\meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.pdb\" /Fopandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.obj \"/c\" pandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/ccalendar.pyx.c\n",
            "      pandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/ccalendar.pyx.c(7550): error C2198: '_PyLong_AsByteArray': too few arguments for call\n",
            "      pandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/ccalendar.pyx.c(7860): error C2198: '_PyLong_AsByteArray': too few arguments for call\n",
            "      [46/151] Compiling C object pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_base.pyx.c.obj\n",
            "      \u001b[31mFAILED: [code=2] \u001b[0mpandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_base.pyx.c.obj\n",
            "      \"cl\" \"-Ipandas\\_libs\\tslibs\\base.cp313-win_amd64.pyd.p\" \"-Ipandas\\_libs\\tslibs\" \"-I..\\..\\pandas\\_libs\\tslibs\" \"-I..\\..\\..\\..\\pip-build-env-trsqc7t6\\overlay\\Lib\\site-packages\\numpy\\_core\\include\" \"-I..\\..\\pandas\\_libs\\include\" \"-IC:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python313\\Include\" \"-DNDEBUG\" \"/MD\" \"/nologo\" \"/showIncludes\" \"/utf-8\" \"-w\" \"/O2\" \"/Gw\" \"-DNPY_NO_DEPRECATED_API=0\" \"-DNPY_TARGET_VERSION=NPY_1_21_API_VERSION\" \"/Fdpandas\\_libs\\tslibs\\base.cp313-win_amd64.pyd.p\\meson-generated_pandas__libs_tslibs_base.pyx.c.pdb\" /Fopandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_base.pyx.c.obj \"/c\" pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/base.pyx.c\n",
            "      pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/base.pyx.c(5397): error C2198: '_PyLong_AsByteArray': too few arguments for call\n",
            "      pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/base.pyx.c(5631): error C2198: '_PyLong_AsByteArray': too few arguments for call\n",
            "      [47/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/tslib.pyx\n",
            "      [48/151] Compiling C object pandas/_libs/tslibs/dtypes.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_dtypes.pyx.c.obj\n",
            "      \u001b[31mFAILED: [code=2] \u001b[0mpandas/_libs/tslibs/dtypes.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_dtypes.pyx.c.obj\n",
            "      \"cl\" \"-Ipandas\\_libs\\tslibs\\dtypes.cp313-win_amd64.pyd.p\" \"-Ipandas\\_libs\\tslibs\" \"-I..\\..\\pandas\\_libs\\tslibs\" \"-I..\\..\\..\\..\\pip-build-env-trsqc7t6\\overlay\\Lib\\site-packages\\numpy\\_core\\include\" \"-I..\\..\\pandas\\_libs\\include\" \"-IC:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python313\\Include\" \"-DNDEBUG\" \"/MD\" \"/nologo\" \"/showIncludes\" \"/utf-8\" \"-w\" \"/O2\" \"/Gw\" \"-DNPY_NO_DEPRECATED_API=0\" \"-DNPY_TARGET_VERSION=NPY_1_21_API_VERSION\" \"/Fdpandas\\_libs\\tslibs\\dtypes.cp313-win_amd64.pyd.p\\meson-generated_pandas__libs_tslibs_dtypes.pyx.c.pdb\" /Fopandas/_libs/tslibs/dtypes.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_dtypes.pyx.c.obj \"/c\" pandas/_libs/tslibs/dtypes.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/dtypes.pyx.c\n",
            "      pandas/_libs/tslibs/dtypes.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/dtypes.pyx.c(16553): error C2198: '_PyLong_AsByteArray': too few arguments for call\n",
            "      pandas/_libs/tslibs/dtypes.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/dtypes.pyx.c(16749): error C2198: '_PyLong_AsByteArray': too few arguments for call\n",
            "      pandas/_libs/tslibs/dtypes.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/dtypes.pyx.c(17097): error C2198: '_PyLong_AsByteArray': too few arguments for call\n",
            "      pandas/_libs/tslibs/dtypes.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/dtypes.pyx.c(17369): error C2198: '_PyLong_AsByteArray': too few arguments for call\n",
            "      pandas/_libs/tslibs/dtypes.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/dtypes.pyx.c(17603): error C2198: '_PyLong_AsByteArray': too few arguments for call\n",
            "      [49/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/writers.pyx\n",
            "      [50/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/window/aggregations.pyx\n",
            "      [51/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/window/indexers.pyx\n",
            "      [52/151] Compiling Cython source C:/Users/Alex/AppData/Local/Temp/pip-install-r_fss5y_/pandas_ba725ff0742849a39cdb1105567dddae/pandas/_libs/sparse.pyx\n",
            "      ninja: build stopped: subcommand failed.\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "Ã— Encountered error while generating package metadata.\n",
            "â•°â”€> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xgboost==2.0.0\n",
            "  Downloading xgboost-2.0.0-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
            "Collecting numpy (from xgboost==2.0.0)\n",
            "  Using cached numpy-2.3.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
            "Collecting scipy (from xgboost==2.0.0)\n",
            "  Using cached scipy-1.16.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
            "Downloading xgboost-2.0.0-py3-none-win_amd64.whl (99.7 MB)\n",
            "   ---------------------------------------- 0.0/99.7 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.5/99.7 MB 3.2 MB/s eta 0:00:32\n",
            "   ---------------------------------------- 1.0/99.7 MB 3.1 MB/s eta 0:00:32\n",
            "    --------------------------------------- 1.8/99.7 MB 3.1 MB/s eta 0:00:32\n",
            "    --------------------------------------- 2.4/99.7 MB 3.1 MB/s eta 0:00:32\n",
            "   - -------------------------------------- 3.1/99.7 MB 3.1 MB/s eta 0:00:31\n",
            "   - -------------------------------------- 3.7/99.7 MB 3.1 MB/s eta 0:00:31\n",
            "   - -------------------------------------- 4.5/99.7 MB 3.1 MB/s eta 0:00:31\n",
            "   - -------------------------------------- 5.0/99.7 MB 3.1 MB/s eta 0:00:31\n",
            "   -- ------------------------------------- 5.8/99.7 MB 3.1 MB/s eta 0:00:31\n",
            "   -- ------------------------------------- 6.3/99.7 MB 3.1 MB/s eta 0:00:30\n",
            "   -- ------------------------------------- 6.8/99.7 MB 3.1 MB/s eta 0:00:30\n",
            "   --- ------------------------------------ 7.6/99.7 MB 3.1 MB/s eta 0:00:30\n",
            "   --- ------------------------------------ 8.1/99.7 MB 3.1 MB/s eta 0:00:30\n",
            "   --- ------------------------------------ 8.9/99.7 MB 3.1 MB/s eta 0:00:30\n",
            "   --- ------------------------------------ 9.4/99.7 MB 3.1 MB/s eta 0:00:29\n",
            "   ---- ----------------------------------- 10.2/99.7 MB 3.1 MB/s eta 0:00:29\n",
            "   ---- ----------------------------------- 10.7/99.7 MB 3.1 MB/s eta 0:00:29\n",
            "   ---- ----------------------------------- 11.3/99.7 MB 3.1 MB/s eta 0:00:29\n",
            "   ---- ----------------------------------- 12.1/99.7 MB 3.1 MB/s eta 0:00:29\n",
            "   ----- ---------------------------------- 12.6/99.7 MB 3.1 MB/s eta 0:00:28\n",
            "   ----- ---------------------------------- 13.4/99.7 MB 3.1 MB/s eta 0:00:28\n",
            "   ----- ---------------------------------- 13.9/99.7 MB 3.1 MB/s eta 0:00:28\n",
            "   ----- ---------------------------------- 14.7/99.7 MB 3.1 MB/s eta 0:00:28\n",
            "   ----- ---------------------------------- 14.9/99.7 MB 3.1 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 15.5/99.7 MB 3.1 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 16.3/99.7 MB 3.1 MB/s eta 0:00:28\n",
            "   ------ --------------------------------- 16.8/99.7 MB 3.1 MB/s eta 0:00:28\n",
            "   ------- -------------------------------- 17.6/99.7 MB 3.1 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 18.1/99.7 MB 3.1 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 18.6/99.7 MB 3.1 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 19.4/99.7 MB 3.1 MB/s eta 0:00:27\n",
            "   ------- -------------------------------- 19.9/99.7 MB 3.1 MB/s eta 0:00:27\n",
            "   -------- ------------------------------- 20.4/99.7 MB 3.1 MB/s eta 0:00:26\n",
            "   -------- ------------------------------- 21.2/99.7 MB 3.1 MB/s eta 0:00:26\n",
            "   -------- ------------------------------- 21.8/99.7 MB 3.1 MB/s eta 0:00:26\n",
            "   --------- ------------------------------ 22.5/99.7 MB 3.1 MB/s eta 0:00:26\n",
            "   --------- ------------------------------ 23.1/99.7 MB 3.1 MB/s eta 0:00:25\n",
            "   --------- ------------------------------ 23.9/99.7 MB 3.1 MB/s eta 0:00:25\n",
            "   --------- ------------------------------ 24.4/99.7 MB 3.1 MB/s eta 0:00:25\n",
            "   ---------- ----------------------------- 25.2/99.7 MB 3.1 MB/s eta 0:00:25\n",
            "   ---------- ----------------------------- 25.7/99.7 MB 3.1 MB/s eta 0:00:25\n",
            "   ---------- ----------------------------- 26.2/99.7 MB 3.1 MB/s eta 0:00:24\n",
            "   ---------- ----------------------------- 27.0/99.7 MB 3.1 MB/s eta 0:00:24\n",
            "   ----------- ---------------------------- 27.5/99.7 MB 3.1 MB/s eta 0:00:24\n",
            "   ----------- ---------------------------- 28.3/99.7 MB 3.1 MB/s eta 0:00:24\n",
            "   ----------- ---------------------------- 28.8/99.7 MB 3.1 MB/s eta 0:00:23\n",
            "   ----------- ---------------------------- 29.6/99.7 MB 3.1 MB/s eta 0:00:23\n",
            "   ------------ --------------------------- 30.1/99.7 MB 3.1 MB/s eta 0:00:23\n",
            "   ------------ --------------------------- 30.7/99.7 MB 3.1 MB/s eta 0:00:23\n",
            "   ------------ --------------------------- 31.5/99.7 MB 3.1 MB/s eta 0:00:23\n",
            "   ------------ --------------------------- 32.0/99.7 MB 3.1 MB/s eta 0:00:22\n",
            "   ------------- -------------------------- 32.8/99.7 MB 3.1 MB/s eta 0:00:22\n",
            "   ------------- -------------------------- 33.3/99.7 MB 3.1 MB/s eta 0:00:22\n",
            "   ------------- -------------------------- 34.1/99.7 MB 3.1 MB/s eta 0:00:22\n",
            "   ------------- -------------------------- 34.6/99.7 MB 3.1 MB/s eta 0:00:22\n",
            "   -------------- ------------------------- 35.4/99.7 MB 3.1 MB/s eta 0:00:21\n",
            "   -------------- ------------------------- 35.9/99.7 MB 3.1 MB/s eta 0:00:21\n",
            "   -------------- ------------------------- 36.7/99.7 MB 3.1 MB/s eta 0:00:21\n",
            "   -------------- ------------------------- 37.2/99.7 MB 3.1 MB/s eta 0:00:21\n",
            "   --------------- ------------------------ 37.7/99.7 MB 3.1 MB/s eta 0:00:21\n",
            "   --------------- ------------------------ 38.5/99.7 MB 3.1 MB/s eta 0:00:20\n",
            "   --------------- ------------------------ 39.1/99.7 MB 3.1 MB/s eta 0:00:20\n",
            "   --------------- ------------------------ 39.8/99.7 MB 3.1 MB/s eta 0:00:20\n",
            "   ---------------- ----------------------- 40.4/99.7 MB 3.1 MB/s eta 0:00:20\n",
            "   ---------------- ----------------------- 41.2/99.7 MB 3.1 MB/s eta 0:00:19\n",
            "   ---------------- ----------------------- 41.7/99.7 MB 3.1 MB/s eta 0:00:19\n",
            "   ---------------- ----------------------- 42.2/99.7 MB 3.1 MB/s eta 0:00:19\n",
            "   ----------------- ---------------------- 43.0/99.7 MB 3.1 MB/s eta 0:00:19\n",
            "   ----------------- ---------------------- 43.5/99.7 MB 3.1 MB/s eta 0:00:19\n",
            "   ----------------- ---------------------- 44.3/99.7 MB 3.1 MB/s eta 0:00:18\n",
            "   ----------------- ---------------------- 44.8/99.7 MB 3.1 MB/s eta 0:00:18\n",
            "   ------------------ --------------------- 45.6/99.7 MB 3.1 MB/s eta 0:00:18\n",
            "   ------------------ --------------------- 45.9/99.7 MB 3.1 MB/s eta 0:00:18\n",
            "   ------------------ --------------------- 46.4/99.7 MB 3.1 MB/s eta 0:00:18\n",
            "   ------------------ --------------------- 47.2/99.7 MB 3.1 MB/s eta 0:00:18\n",
            "   ------------------- -------------------- 47.4/99.7 MB 3.1 MB/s eta 0:00:17\n",
            "   ------------------- -------------------- 48.5/99.7 MB 3.1 MB/s eta 0:00:17\n",
            "   ------------------- -------------------- 49.0/99.7 MB 3.1 MB/s eta 0:00:17\n",
            "   ------------------- -------------------- 49.8/99.7 MB 3.1 MB/s eta 0:00:17\n",
            "   -------------------- ------------------- 50.3/99.7 MB 3.1 MB/s eta 0:00:16\n",
            "   -------------------- ------------------- 51.1/99.7 MB 3.1 MB/s eta 0:00:16\n",
            "   -------------------- ------------------- 51.6/99.7 MB 3.1 MB/s eta 0:00:16\n",
            "   --------------------- ------------------ 52.4/99.7 MB 3.1 MB/s eta 0:00:16\n",
            "   --------------------- ------------------ 53.0/99.7 MB 3.1 MB/s eta 0:00:16\n",
            "   --------------------- ------------------ 53.7/99.7 MB 3.1 MB/s eta 0:00:15\n",
            "   --------------------- ------------------ 54.3/99.7 MB 3.1 MB/s eta 0:00:15\n",
            "   ---------------------- ----------------- 55.1/99.7 MB 3.1 MB/s eta 0:00:15\n",
            "   ---------------------- ----------------- 55.6/99.7 MB 3.1 MB/s eta 0:00:15\n",
            "   ---------------------- ----------------- 56.1/99.7 MB 3.1 MB/s eta 0:00:15\n",
            "   ---------------------- ----------------- 56.9/99.7 MB 3.1 MB/s eta 0:00:14\n",
            "   ----------------------- ---------------- 57.4/99.7 MB 3.1 MB/s eta 0:00:14\n",
            "   ----------------------- ---------------- 58.2/99.7 MB 3.1 MB/s eta 0:00:14\n",
            "   ----------------------- ---------------- 58.7/99.7 MB 3.1 MB/s eta 0:00:14\n",
            "   ----------------------- ---------------- 59.5/99.7 MB 3.1 MB/s eta 0:00:13\n",
            "   ------------------------ --------------- 60.0/99.7 MB 3.1 MB/s eta 0:00:13\n",
            "   ------------------------ --------------- 60.8/99.7 MB 3.1 MB/s eta 0:00:13\n",
            "   ------------------------ --------------- 61.3/99.7 MB 3.1 MB/s eta 0:00:13\n",
            "   ------------------------ --------------- 62.1/99.7 MB 3.1 MB/s eta 0:00:13\n",
            "   ------------------------- -------------- 62.7/99.7 MB 3.1 MB/s eta 0:00:12\n",
            "   ------------------------- -------------- 63.4/99.7 MB 3.1 MB/s eta 0:00:12\n",
            "   ------------------------- -------------- 64.0/99.7 MB 3.1 MB/s eta 0:00:12\n",
            "   ------------------------- -------------- 64.7/99.7 MB 3.1 MB/s eta 0:00:12\n",
            "   -------------------------- ------------- 65.3/99.7 MB 3.1 MB/s eta 0:00:12\n",
            "   -------------------------- ------------- 66.1/99.7 MB 3.1 MB/s eta 0:00:11\n",
            "   -------------------------- ------------- 66.6/99.7 MB 3.1 MB/s eta 0:00:11\n",
            "   -------------------------- ------------- 67.1/99.7 MB 3.1 MB/s eta 0:00:11\n",
            "   --------------------------- ------------ 67.9/99.7 MB 3.1 MB/s eta 0:00:11\n",
            "   --------------------------- ------------ 68.4/99.7 MB 3.1 MB/s eta 0:00:11\n",
            "   --------------------------- ------------ 69.2/99.7 MB 3.1 MB/s eta 0:00:10\n",
            "   --------------------------- ------------ 69.7/99.7 MB 3.1 MB/s eta 0:00:10\n",
            "   ---------------------------- ----------- 70.5/99.7 MB 3.1 MB/s eta 0:00:10\n",
            "   ---------------------------- ----------- 71.0/99.7 MB 3.1 MB/s eta 0:00:10\n",
            "   ---------------------------- ----------- 71.8/99.7 MB 3.1 MB/s eta 0:00:09\n",
            "   ----------------------------- ---------- 72.4/99.7 MB 3.1 MB/s eta 0:00:09\n",
            "   ----------------------------- ---------- 72.9/99.7 MB 3.1 MB/s eta 0:00:09\n",
            "   ----------------------------- ---------- 73.7/99.7 MB 3.1 MB/s eta 0:00:09\n",
            "   ----------------------------- ---------- 74.4/99.7 MB 3.1 MB/s eta 0:00:09\n",
            "   ------------------------------ --------- 75.0/99.7 MB 3.1 MB/s eta 0:00:08\n",
            "   ------------------------------ --------- 75.5/99.7 MB 3.1 MB/s eta 0:00:08\n",
            "   ------------------------------ --------- 76.3/99.7 MB 3.1 MB/s eta 0:00:08\n",
            "   ------------------------------ --------- 76.8/99.7 MB 3.1 MB/s eta 0:00:08\n",
            "   ------------------------------- -------- 77.6/99.7 MB 3.1 MB/s eta 0:00:08\n",
            "   ------------------------------- -------- 78.1/99.7 MB 3.1 MB/s eta 0:00:07\n",
            "   ------------------------------- -------- 78.9/99.7 MB 3.1 MB/s eta 0:00:07\n",
            "   ------------------------------- -------- 79.2/99.7 MB 3.1 MB/s eta 0:00:07\n",
            "   -------------------------------- ------- 80.2/99.7 MB 3.1 MB/s eta 0:00:07\n",
            "   -------------------------------- ------- 80.7/99.7 MB 3.1 MB/s eta 0:00:07\n",
            "   -------------------------------- ------- 81.5/99.7 MB 3.1 MB/s eta 0:00:06\n",
            "   -------------------------------- ------- 82.1/99.7 MB 3.1 MB/s eta 0:00:06\n",
            "   --------------------------------- ------ 82.8/99.7 MB 3.1 MB/s eta 0:00:06\n",
            "   --------------------------------- ------ 83.4/99.7 MB 3.1 MB/s eta 0:00:06\n",
            "   --------------------------------- ------ 84.1/99.7 MB 3.1 MB/s eta 0:00:06\n",
            "   --------------------------------- ------ 84.7/99.7 MB 3.1 MB/s eta 0:00:05\n",
            "   ---------------------------------- ----- 85.5/99.7 MB 3.1 MB/s eta 0:00:05\n",
            "   ---------------------------------- ----- 86.0/99.7 MB 3.1 MB/s eta 0:00:05\n",
            "   ---------------------------------- ----- 86.8/99.7 MB 3.1 MB/s eta 0:00:05\n",
            "   ----------------------------------- ---- 87.3/99.7 MB 3.1 MB/s eta 0:00:05\n",
            "   ----------------------------------- ---- 88.1/99.7 MB 3.1 MB/s eta 0:00:04\n",
            "   ----------------------------------- ---- 88.6/99.7 MB 3.1 MB/s eta 0:00:04\n",
            "   ----------------------------------- ---- 89.1/99.7 MB 3.1 MB/s eta 0:00:04\n",
            "   ------------------------------------ --- 89.9/99.7 MB 3.1 MB/s eta 0:00:04\n",
            "   ------------------------------------ --- 90.4/99.7 MB 3.1 MB/s eta 0:00:03\n",
            "   ------------------------------------ --- 91.2/99.7 MB 3.1 MB/s eta 0:00:03\n",
            "   ------------------------------------ --- 91.8/99.7 MB 3.1 MB/s eta 0:00:03\n",
            "   ------------------------------------- -- 92.5/99.7 MB 3.1 MB/s eta 0:00:03\n",
            "   ------------------------------------- -- 93.1/99.7 MB 3.1 MB/s eta 0:00:03\n",
            "   ------------------------------------- -- 93.6/99.7 MB 3.1 MB/s eta 0:00:02\n",
            "   ------------------------------------- -- 94.4/99.7 MB 3.1 MB/s eta 0:00:02\n",
            "   -------------------------------------- - 94.9/99.7 MB 3.1 MB/s eta 0:00:02\n",
            "   -------------------------------------- - 95.7/99.7 MB 3.1 MB/s eta 0:00:02\n",
            "   -------------------------------------- - 96.2/99.7 MB 3.1 MB/s eta 0:00:02\n",
            "   -------------------------------------- - 97.0/99.7 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  97.5/99.7 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  98.3/99.7 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  98.8/99.7 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  99.6/99.7 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 99.7/99.7 MB 3.1 MB/s  0:00:32\n",
            "Using cached numpy-2.3.2-cp313-cp313-win_amd64.whl (12.8 MB)\n",
            "Using cached scipy-1.16.1-cp313-cp313-win_amd64.whl (38.5 MB)\n",
            "Installing collected packages: numpy, scipy, xgboost\n",
            "\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ---------------------------------------- 0/3 [numpy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   ------------- -------------------------- 1/3 [scipy]\n",
            "   -------------------------- ------------- 2/3 [xgboost]\n",
            "   -------------------------- ------------- 2/3 [xgboost]\n",
            "   -------------------------- ------------- 2/3 [xgboost]\n",
            "   -------------------------- ------------- 2/3 [xgboost]\n",
            "   -------------------------- ------------- 2/3 [xgboost]\n",
            "   -------------------------- ------------- 2/3 [xgboost]\n",
            "   -------------------------- ------------- 2/3 [xgboost]\n",
            "   -------------------------- ------------- 2/3 [xgboost]\n",
            "   -------------------------- ------------- 2/3 [xgboost]\n",
            "   ---------------------------------------- 3/3 [xgboost]\n",
            "\n",
            "Successfully installed numpy-2.3.2 scipy-1.16.1 xgboost-2.0.0\n",
            "Collecting tqdm==4.66.1\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\alex\\documents\\code\\cossy179\\nba-machine-learning-sports-betting\\.venv\\lib\\site-packages (from tqdm==4.66.1) (0.4.6)\n",
            "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm\n",
            "Successfully installed tqdm-4.66.1\n",
            "Collecting flask==3.0.0\n",
            "  Downloading flask-3.0.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting Werkzeug>=3.0.0 (from flask==3.0.0)\n",
            "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Jinja2>=3.1.2 (from flask==3.0.0)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting itsdangerous>=2.1.2 (from flask==3.0.0)\n",
            "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting click>=8.1.3 (from flask==3.0.0)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting blinker>=1.6.2 (from flask==3.0.0)\n",
            "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\alex\\documents\\code\\cossy179\\nba-machine-learning-sports-betting\\.venv\\lib\\site-packages (from click>=8.1.3->flask==3.0.0) (0.4.6)\n",
            "Collecting MarkupSafe>=2.0 (from Jinja2>=3.1.2->flask==3.0.0)\n",
            "  Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
            "Downloading flask-3.0.0-py3-none-any.whl (99 kB)\n",
            "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
            "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl (15 kB)\n",
            "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Installing collected packages: MarkupSafe, itsdangerous, click, blinker, Werkzeug, Jinja2, flask\n",
            "\n",
            "   ----------- ---------------------------- 2/7 [click]\n",
            "   ----------- ---------------------------- 2/7 [click]\n",
            "   ---------------------- ----------------- 4/7 [Werkzeug]\n",
            "   ---------------------- ----------------- 4/7 [Werkzeug]\n",
            "   ---------------------- ----------------- 4/7 [Werkzeug]\n",
            "   ---------------------- ----------------- 4/7 [Werkzeug]\n",
            "   ---------------------- ----------------- 4/7 [Werkzeug]\n",
            "   ---------------------------- ----------- 5/7 [Jinja2]\n",
            "   ---------------------------- ----------- 5/7 [Jinja2]\n",
            "   ---------------------------------- ----- 6/7 [flask]\n",
            "   ---------------------------------- ----- 6/7 [flask]\n",
            "   ---------------------------------------- 7/7 [flask]\n",
            "\n",
            "Successfully installed Jinja2-3.1.6 MarkupSafe-3.0.2 Werkzeug-3.1.3 blinker-1.9.0 click-8.2.1 flask-3.0.0 itsdangerous-2.2.0\n",
            "Collecting scikit-learn==1.3.1\n",
            "  Downloading scikit-learn-1.3.1.tar.gz (7.5 MB)\n",
            "     ---------------------------------------- 0.0/7.5 MB ? eta -:--:--\n",
            "     -- ------------------------------------- 0.5/7.5 MB 3.1 MB/s eta 0:00:03\n",
            "     ----- ---------------------------------- 1.0/7.5 MB 3.1 MB/s eta 0:00:03\n",
            "     --------- ------------------------------ 1.8/7.5 MB 3.1 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 2.4/7.5 MB 3.1 MB/s eta 0:00:02\n",
            "     ---------------- ----------------------- 3.1/7.5 MB 3.1 MB/s eta 0:00:02\n",
            "     ------------------- -------------------- 3.7/7.5 MB 3.1 MB/s eta 0:00:02\n",
            "     ----------------------- ---------------- 4.5/7.5 MB 3.1 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 5.0/7.5 MB 3.1 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 5.5/7.5 MB 3.1 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 6.0/7.5 MB 3.1 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 6.8/7.5 MB 3.1 MB/s eta 0:00:01\n",
            "     ---------------------------------------  7.3/7.5 MB 3.1 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 7.5/7.5 MB 3.1 MB/s  0:00:02\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): still running...\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  Ã— Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  â”‚ exit code: 1\n",
            "  â•°â”€> [649 lines of output]\n",
            "      Partial import of sklearn during the build process.\n",
            "      test_program.c\n",
            "      Generating code\n",
            "      Finished generating code\n",
            "      test_program.c\n",
            "      LINK : warning LNK4044: unrecognized option '/openmp'; ignored\n",
            "      Generating code\n",
            "      Finished generating code\n",
            "      Compiling sklearn\\__check_build\\_check_build.pyx because it changed.\n",
            "      Compiling sklearn\\_isotonic.pyx because it changed.\n",
            "      Compiling sklearn\\_loss\\_loss.pyx because it changed.\n",
            "      Compiling sklearn\\cluster\\_dbscan_inner.pyx because it changed.\n",
            "      Compiling sklearn\\cluster\\_hierarchical_fast.pyx because it changed.\n",
            "      Compiling sklearn\\cluster\\_k_means_common.pyx because it changed.\n",
            "      Compiling sklearn\\cluster\\_k_means_lloyd.pyx because it changed.\n",
            "      Compiling sklearn\\cluster\\_k_means_elkan.pyx because it changed.\n",
            "      Compiling sklearn\\cluster\\_k_means_minibatch.pyx because it changed.\n",
            "      Compiling sklearn\\cluster\\_hdbscan\\_linkage.pyx because it changed.\n",
            "      Compiling sklearn\\cluster\\_hdbscan\\_reachability.pyx because it changed.\n",
            "      Compiling sklearn\\cluster\\_hdbscan\\_tree.pyx because it changed.\n",
            "      Compiling sklearn\\datasets\\_svmlight_format_fast.pyx because it changed.\n",
            "      Compiling sklearn\\decomposition\\_online_lda_fast.pyx because it changed.\n",
            "      Compiling sklearn\\decomposition\\_cdnmf_fast.pyx because it changed.\n",
            "      Compiling sklearn\\ensemble\\_gradient_boosting.pyx because it changed.\n",
            "      Compiling sklearn\\ensemble\\_hist_gradient_boosting\\_gradient_boosting.pyx because it changed.\n",
            "      Compiling sklearn\\ensemble\\_hist_gradient_boosting\\histogram.pyx because it changed.\n",
            "      Compiling sklearn\\ensemble\\_hist_gradient_boosting\\splitting.pyx because it changed.\n",
            "      Compiling sklearn\\ensemble\\_hist_gradient_boosting\\_binning.pyx because it changed.\n",
            "      Compiling sklearn\\ensemble\\_hist_gradient_boosting\\_predictor.pyx because it changed.\n",
            "      Compiling sklearn\\ensemble\\_hist_gradient_boosting\\_bitset.pyx because it changed.\n",
            "      Compiling sklearn\\ensemble\\_hist_gradient_boosting\\common.pyx because it changed.\n",
            "      Compiling sklearn\\ensemble\\_hist_gradient_boosting\\utils.pyx because it changed.\n",
            "      Compiling sklearn\\feature_extraction\\_hashing_fast.pyx because it changed.\n",
            "      Compiling sklearn\\linear_model\\_cd_fast.pyx because it changed.\n",
            "      Compiling sklearn\\linear_model\\_sgd_fast.pyx because it changed.\n",
            "      Compiling sklearn\\linear_model\\_sag_fast.pyx because it changed.\n",
            "      Compiling sklearn\\manifold\\_utils.pyx because it changed.\n",
            "      Compiling sklearn\\manifold\\_barnes_hut_tsne.pyx because it changed.\n",
            "      Compiling sklearn\\metrics\\_pairwise_fast.pyx because it changed.\n",
            "      Compiling sklearn\\metrics\\_dist_metrics.pyx because it changed.\n",
            "      Compiling sklearn\\metrics\\cluster\\_expected_mutual_info_fast.pyx because it changed.\n",
            "      Compiling sklearn\\metrics\\_pairwise_distances_reduction\\_datasets_pair.pyx because it changed.\n",
            "      Compiling sklearn\\metrics\\_pairwise_distances_reduction\\_middle_term_computer.pyx because it changed.\n",
            "      Compiling sklearn\\metrics\\_pairwise_distances_reduction\\_base.pyx because it changed.\n",
            "      Compiling sklearn\\metrics\\_pairwise_distances_reduction\\_argkmin.pyx because it changed.\n",
            "      Compiling sklearn\\metrics\\_pairwise_distances_reduction\\_argkmin_classmode.pyx because it changed.\n",
            "      Compiling sklearn\\metrics\\_pairwise_distances_reduction\\_radius_neighbors.pyx because it changed.\n",
            "      Compiling sklearn\\preprocessing\\_csr_polynomial_expansion.pyx because it changed.\n",
            "      Compiling sklearn\\preprocessing\\_target_encoder_fast.pyx because it changed.\n",
            "      Compiling sklearn\\neighbors\\_ball_tree.pyx because it changed.\n",
            "      Compiling sklearn\\neighbors\\_kd_tree.pyx because it changed.\n",
            "      Compiling sklearn\\neighbors\\_partition_nodes.pyx because it changed.\n",
            "      Compiling sklearn\\neighbors\\_quad_tree.pyx because it changed.\n",
            "      Compiling sklearn\\svm\\_newrand.pyx because it changed.\n",
            "      Compiling sklearn\\svm\\_libsvm.pyx because it changed.\n",
            "      Compiling sklearn\\svm\\_liblinear.pyx because it changed.\n",
            "      Compiling sklearn\\svm\\_libsvm_sparse.pyx because it changed.\n",
            "      Compiling sklearn\\tree\\_tree.pyx because it changed.\n",
            "      Compiling sklearn\\tree\\_splitter.pyx because it changed.\n",
            "      Compiling sklearn\\tree\\_criterion.pyx because it changed.\n",
            "      Compiling sklearn\\tree\\_utils.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\sparsefuncs_fast.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_cython_blas.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\arrayfuncs.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\murmurhash.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_fast_dict.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_openmp_helpers.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_seq_dataset.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_weight_vector.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_random.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_logistic_sigmoid.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_typedefs.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_heap.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_sorting.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_vector_sentinel.pyx because it changed.\n",
            "      Compiling sklearn\\utils\\_isfinite.pyx because it changed.\n",
            "      [ 1/68] Cythonizing sklearn\\__check_build\\_check_build.pyx\n",
            "      [ 2/68] Cythonizing sklearn\\_isotonic.pyx\n",
            "      [ 3/68] Cythonizing sklearn\\_loss\\_loss.pyx\n",
            "      [ 4/68] Cythonizing sklearn\\cluster\\_dbscan_inner.pyx\n",
            "      [ 5/68] Cythonizing sklearn\\cluster\\_hdbscan\\_linkage.pyx\n",
            "      [ 6/68] Cythonizing sklearn\\cluster\\_hdbscan\\_reachability.pyx\n",
            "      [ 7/68] Cythonizing sklearn\\cluster\\_hdbscan\\_tree.pyx\n",
            "      [ 8/68] Cythonizing sklearn\\cluster\\_hierarchical_fast.pyx\n",
            "      [ 9/68] Cythonizing sklearn\\cluster\\_k_means_common.pyx\n",
            "      [10/68] Cythonizing sklearn\\cluster\\_k_means_elkan.pyx\n",
            "      [11/68] Cythonizing sklearn\\cluster\\_k_means_lloyd.pyx\n",
            "      [12/68] Cythonizing sklearn\\cluster\\_k_means_minibatch.pyx\n",
            "      [13/68] Cythonizing sklearn\\datasets\\_svmlight_format_fast.pyx\n",
            "      [14/68] Cythonizing sklearn\\decomposition\\_cdnmf_fast.pyx\n",
            "      [15/68] Cythonizing sklearn\\decomposition\\_online_lda_fast.pyx\n",
            "      [16/68] Cythonizing sklearn\\ensemble\\_gradient_boosting.pyx\n",
            "      [17/68] Cythonizing sklearn\\ensemble\\_hist_gradient_boosting\\_binning.pyx\n",
            "      [18/68] Cythonizing sklearn\\ensemble\\_hist_gradient_boosting\\_bitset.pyx\n",
            "      [19/68] Cythonizing sklearn\\ensemble\\_hist_gradient_boosting\\_gradient_boosting.pyx\n",
            "      [20/68] Cythonizing sklearn\\ensemble\\_hist_gradient_boosting\\_predictor.pyx\n",
            "      [21/68] Cythonizing sklearn\\ensemble\\_hist_gradient_boosting\\common.pyx\n",
            "      [22/68] Cythonizing sklearn\\ensemble\\_hist_gradient_boosting\\histogram.pyx\n",
            "      [23/68] Cythonizing sklearn\\ensemble\\_hist_gradient_boosting\\splitting.pyx\n",
            "      [24/68] Cythonizing sklearn\\ensemble\\_hist_gradient_boosting\\utils.pyx\n",
            "      [25/68] Cythonizing sklearn\\feature_extraction\\_hashing_fast.pyx\n",
            "      [26/68] Cythonizing sklearn\\linear_model\\_cd_fast.pyx\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          # particularly tiny on Windows/MSVC.\n",
            "          # It corresponds to the maximum representable value for\n",
            "          # 32-bit signed integers (i.e. 2^31 - 1).\n",
            "          RAND_R_MAX = 2147483647\n",
            "      \n",
            "      cpdef sample_without_replacement(cnp.int_t n_population,\n",
            "                                      ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pxd:19:33: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          # It corresponds to the maximum representable value for\n",
            "          # 32-bit signed integers (i.e. 2^31 - 1).\n",
            "          RAND_R_MAX = 2147483647\n",
            "      \n",
            "      cpdef sample_without_replacement(cnp.int_t n_population,\n",
            "                                       cnp.int_t n_samples,\n",
            "                                      ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pxd:20:33: 'int_t' is not a type identifier\n",
            "      Traceback (most recent call last):\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1262, in cythonize_one_helper\n",
            "          return cythonize_one(*m)\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1238, in cythonize_one\n",
            "          raise CompileError(None, pyx_file)\n",
            "      Cython.Compiler.Errors.CompileError: sklearn\\linear_model\\_cd_fast.pyx\n",
            "      [27/68] Cythonizing sklearn\\linear_model\\_sag_fast.pyx\n",
            "      [28/68] Cythonizing sklearn\\linear_model\\_sgd_fast.pyx\n",
            "      [29/68] Cythonizing sklearn\\manifold\\_barnes_hut_tsne.pyx\n",
            "      [30/68] Cythonizing sklearn\\manifold\\_utils.pyx\n",
            "      [31/68] Cythonizing sklearn\\metrics\\_dist_metrics.pyx\n",
            "      [32/68] Cythonizing sklearn\\metrics\\_pairwise_distances_reduction\\_argkmin.pyx\n",
            "      [33/68] Cythonizing sklearn\\metrics\\_pairwise_distances_reduction\\_argkmin_classmode.pyx\n",
            "      [34/68] Cythonizing sklearn\\metrics\\_pairwise_distances_reduction\\_base.pyx\n",
            "      [35/68] Cythonizing sklearn\\metrics\\_pairwise_distances_reduction\\_datasets_pair.pyx\n",
            "      [36/68] Cythonizing sklearn\\metrics\\_pairwise_distances_reduction\\_middle_term_computer.pyx\n",
            "      [37/68] Cythonizing sklearn\\metrics\\_pairwise_distances_reduction\\_radius_neighbors.pyx\n",
            "      [38/68] Cythonizing sklearn\\metrics\\_pairwise_fast.pyx\n",
            "      [39/68] Cythonizing sklearn\\metrics\\cluster\\_expected_mutual_info_fast.pyx\n",
            "      [40/68] Cythonizing sklearn\\neighbors\\_ball_tree.pyx\n",
            "      [41/68] Cythonizing sklearn\\neighbors\\_kd_tree.pyx\n",
            "      [42/68] Cythonizing sklearn\\neighbors\\_partition_nodes.pyx\n",
            "      [43/68] Cythonizing sklearn\\neighbors\\_quad_tree.pyx\n",
            "      [44/68] Cythonizing sklearn\\preprocessing\\_csr_polynomial_expansion.pyx\n",
            "      [45/68] Cythonizing sklearn\\preprocessing\\_target_encoder_fast.pyx\n",
            "      [46/68] Cythonizing sklearn\\svm\\_liblinear.pyx\n",
            "      [47/68] Cythonizing sklearn\\svm\\_libsvm.pyx\n",
            "      [48/68] Cythonizing sklearn\\svm\\_libsvm_sparse.pyx\n",
            "      [49/68] Cythonizing sklearn\\svm\\_newrand.pyx\n",
            "      [50/68] Cythonizing sklearn\\tree\\_criterion.pyx\n",
            "      [51/68] Cythonizing sklearn\\tree\\_splitter.pyx\n",
            "      [52/68] Cythonizing sklearn\\tree\\_tree.pyx\n",
            "      [53/68] Cythonizing sklearn\\tree\\_utils.pyx\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          # particularly tiny on Windows/MSVC.\n",
            "          # It corresponds to the maximum representable value for\n",
            "          # 32-bit signed integers (i.e. 2^31 - 1).\n",
            "          RAND_R_MAX = 2147483647\n",
            "      \n",
            "      cpdef sample_without_replacement(cnp.int_t n_population,\n",
            "                                      ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pxd:19:33: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          # It corresponds to the maximum representable value for\n",
            "          # 32-bit signed integers (i.e. 2^31 - 1).\n",
            "          RAND_R_MAX = 2147483647\n",
            "      \n",
            "      cpdef sample_without_replacement(cnp.int_t n_population,\n",
            "                                       cnp.int_t n_samples,\n",
            "                                      ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pxd:20:33: 'int_t' is not a type identifier\n",
            "      Traceback (most recent call last):\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1262, in cythonize_one_helper\n",
            "          return cythonize_one(*m)\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1238, in cythonize_one\n",
            "          raise CompileError(None, pyx_file)\n",
            "      Cython.Compiler.Errors.CompileError: sklearn\\tree\\_utils.pyx\n",
            "      [54/68] Cythonizing sklearn\\utils\\_cython_blas.pyx\n",
            "      [55/68] Cythonizing sklearn\\utils\\_fast_dict.pyx\n",
            "      [56/68] Cythonizing sklearn\\utils\\_heap.pyx\n",
            "      [57/68] Cythonizing sklearn\\utils\\_isfinite.pyx\n",
            "      [58/68] Cythonizing sklearn\\utils\\_logistic_sigmoid.pyx\n",
            "      [59/68] Cythonizing sklearn\\utils\\_openmp_helpers.pyx\n",
            "      [60/68] Cythonizing sklearn\\utils\\_random.pyx\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          # particularly tiny on Windows/MSVC.\n",
            "          # It corresponds to the maximum representable value for\n",
            "          # 32-bit signed integers (i.e. 2^31 - 1).\n",
            "          RAND_R_MAX = 2147483647\n",
            "      \n",
            "      cpdef sample_without_replacement(cnp.int_t n_population,\n",
            "                                      ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pxd:19:33: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          # It corresponds to the maximum representable value for\n",
            "          # 32-bit signed integers (i.e. 2^31 - 1).\n",
            "          RAND_R_MAX = 2147483647\n",
            "      \n",
            "      cpdef sample_without_replacement(cnp.int_t n_population,\n",
            "                                       cnp.int_t n_samples,\n",
            "                                      ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pxd:20:33: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "      from . import check_random_state\n",
            "      \n",
            "      cdef UINT32_t DEFAULT_SEED = 1\n",
            "      \n",
            "      \n",
            "      cpdef _sample_without_replacement_check_input(cnp.int_t n_population,\n",
            "                                                   ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:22:46: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "      \n",
            "      cdef UINT32_t DEFAULT_SEED = 1\n",
            "      \n",
            "      \n",
            "      cpdef _sample_without_replacement_check_input(cnp.int_t n_population,\n",
            "                                                    cnp.int_t n_samples):\n",
            "                                                   ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:23:46: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "                               'n_samples, got n_samples > n_population (%s > %s)'\n",
            "                               % (n_samples, n_population))\n",
            "      \n",
            "      \n",
            "      cpdef _sample_without_replacement_with_tracking_selection(\n",
            "              cnp.int_t n_population,\n",
            "             ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:36:8: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "                               % (n_samples, n_population))\n",
            "      \n",
            "      \n",
            "      cpdef _sample_without_replacement_with_tracking_selection(\n",
            "              cnp.int_t n_population,\n",
            "              cnp.int_t n_samples,\n",
            "             ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:37:8: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "              out[i] = j\n",
            "      \n",
            "          return np.asarray(out)\n",
            "      \n",
            "      \n",
            "      cpdef _sample_without_replacement_with_pool(cnp.int_t n_population,\n",
            "                                                 ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:100:44: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "      \n",
            "          return np.asarray(out)\n",
            "      \n",
            "      \n",
            "      cpdef _sample_without_replacement_with_pool(cnp.int_t n_population,\n",
            "                                                  cnp.int_t n_samples,\n",
            "                                                 ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:101:44: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "      \n",
            "          return np.asarray(out)\n",
            "      \n",
            "      \n",
            "      cpdef _sample_without_replacement_with_reservoir_sampling(\n",
            "          cnp.int_t n_population,\n",
            "         ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:157:4: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          return np.asarray(out)\n",
            "      \n",
            "      \n",
            "      cpdef _sample_without_replacement_with_reservoir_sampling(\n",
            "          cnp.int_t n_population,\n",
            "          cnp.int_t n_samples,\n",
            "         ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:158:4: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "                  out[j] = i\n",
            "      \n",
            "          return np.asarray(out)\n",
            "      \n",
            "      \n",
            "      cpdef sample_without_replacement(cnp.int_t n_population,\n",
            "                                      ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:216:33: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "      \n",
            "          return np.asarray(out)\n",
            "      \n",
            "      \n",
            "      cpdef sample_without_replacement(cnp.int_t n_population,\n",
            "                                       cnp.int_t n_samples,\n",
            "                                      ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:217:33: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          out : ndarray of shape (n_samples,)\n",
            "              The sampled subsets of integer.\n",
            "          \"\"\"\n",
            "          _sample_without_replacement_check_input(n_population, n_samples)\n",
            "      \n",
            "          cdef cnp.int_t i\n",
            "              ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:79:9: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "              The sampled subsets of integer.\n",
            "          \"\"\"\n",
            "          _sample_without_replacement_check_input(n_population, n_samples)\n",
            "      \n",
            "          cdef cnp.int_t i\n",
            "          cdef cnp.int_t j\n",
            "              ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:80:9: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          \"\"\"\n",
            "          _sample_without_replacement_check_input(n_population, n_samples)\n",
            "      \n",
            "          cdef cnp.int_t i\n",
            "          cdef cnp.int_t j\n",
            "          cdef cnp.int_t[::1] out = np.empty((n_samples, ), dtype=int)\n",
            "              ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:81:9: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          out : ndarray of shape (n_samples,)\n",
            "              The sampled subsets of integer.\n",
            "          \"\"\"\n",
            "          _sample_without_replacement_check_input(n_population, n_samples)\n",
            "      \n",
            "          cdef cnp.int_t i\n",
            "              ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:134:9: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "              The sampled subsets of integer.\n",
            "          \"\"\"\n",
            "          _sample_without_replacement_check_input(n_population, n_samples)\n",
            "      \n",
            "          cdef cnp.int_t i\n",
            "          cdef cnp.int_t j\n",
            "              ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:135:9: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          \"\"\"\n",
            "          _sample_without_replacement_check_input(n_population, n_samples)\n",
            "      \n",
            "          cdef cnp.int_t i\n",
            "          cdef cnp.int_t j\n",
            "          cdef cnp.int_t[::1] out = np.empty((n_samples,), dtype=int)\n",
            "              ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:136:9: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          _sample_without_replacement_check_input(n_population, n_samples)\n",
            "      \n",
            "          cdef cnp.int_t i\n",
            "          cdef cnp.int_t j\n",
            "          cdef cnp.int_t[::1] out = np.empty((n_samples,), dtype=int)\n",
            "          cdef cnp.int_t[::1] pool = np.empty((n_population,), dtype=int)\n",
            "              ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:137:9: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "              necessarily random. Use a random permutation of the array if the order\n",
            "              of the items has to be randomized.\n",
            "          \"\"\"\n",
            "          _sample_without_replacement_check_input(n_population, n_samples)\n",
            "      \n",
            "          cdef cnp.int_t i\n",
            "              ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:194:9: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "              of the items has to be randomized.\n",
            "          \"\"\"\n",
            "          _sample_without_replacement_check_input(n_population, n_samples)\n",
            "      \n",
            "          cdef cnp.int_t i\n",
            "          cdef cnp.int_t j\n",
            "              ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:195:9: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          \"\"\"\n",
            "          _sample_without_replacement_check_input(n_population, n_samples)\n",
            "      \n",
            "          cdef cnp.int_t i\n",
            "          cdef cnp.int_t j\n",
            "          cdef cnp.int_t[::1] out = np.empty((n_samples, ), dtype=int)\n",
            "              ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:196:9: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          # 054289.html\n",
            "          #\n",
            "          for i in range(n_samples):\n",
            "              out[i] = i\n",
            "      \n",
            "          for i from n_samples <= i < n_population:\n",
            "         ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pyx:208:4: Compiler crash in AnalyseExpressionsTransform\n",
            "      \n",
            "      ModuleNode.body = StatListNode(_random.pyx:13:0)\n",
            "      StatListNode.stats[8] = StatListNode(_random.pyx:156:6)\n",
            "      StatListNode.stats[0] = CFuncDefNode(_random.pyx:156:6,\n",
            "          args = [...]/3,\n",
            "          doc = 'Sample integers without replacement.\\n\\n    Select n_samples integers from the set [0, n_population) without\\n    replacement.\\n\\n    Time complexity of\\n        O((n_population - n_samples) * O(np.random.randint) + n_samples)\\n    Space complexity of O(n_samples)\\n\\n\\n    Parameters\\n    ----------\\n    n_population : int\\n        The size of the set to sample from.\\n\\n    n_samples : int\\n         The number of integer to sample.\\n\\n    random_state : int, RandomState instance or None, default=None\\n        If int, random_state is the seed used by the random number generator;\\n        If RandomState instance, random_state is the random number generator;\\n        If None, the random number generator is the RandomState instance used\\n        by `np.random`.\\n\\n    Returns\\n    -------\\n    out : ndarray of shape (n_samples,)\\n        The sampled subsets of integer. The order of the items is not\\n        necessarily random. Use a random permutation of the array if the order\\n        of the items has to be randomized.\\n    ',\n",
            "          modifiers = [...]/0,\n",
            "          overridable = 1,\n",
            "          visibility = 'private')\n",
            "      File 'Nodes.py', line 435, in analyse_expressions: StatListNode(_random.pyx:161:4,\n",
            "          is_terminator = True)\n",
            "      File 'Nodes.py', line 6853, in analyse_expressions: ForFromStatNode(_random.pyx:208:4,\n",
            "          relation1 = '<=',\n",
            "          relation2 = '<')\n",
            "      File 'Nodes.py', line 6875, in set_up_loop: ForFromStatNode(_random.pyx:208:4,\n",
            "          relation1 = '<=',\n",
            "          relation2 = '<')\n",
            "      \n",
            "      Compiler crash traceback from this point on:\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Nodes.py\", line 6875, in set_up_loop\n",
            "          loop_type = PyrexTypes.widest_numeric_type(loop_type, self.bound1.type)\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Compiler\\PyrexTypes.py\", line 4481, in widest_numeric_type\n",
            "          elif type1.rank < type2.rank:\n",
            "                            ^^^^^^^^^^\n",
            "      AttributeError: 'ErrorType' object has no attribute 'rank'\n",
            "      Traceback (most recent call last):\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1262, in cythonize_one_helper\n",
            "          return cythonize_one(*m)\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1238, in cythonize_one\n",
            "          raise CompileError(None, pyx_file)\n",
            "      Cython.Compiler.Errors.CompileError: sklearn\\utils\\_random.pyx\n",
            "      [61/68] Cythonizing sklearn\\utils\\_seq_dataset.pyx\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          # particularly tiny on Windows/MSVC.\n",
            "          # It corresponds to the maximum representable value for\n",
            "          # 32-bit signed integers (i.e. 2^31 - 1).\n",
            "          RAND_R_MAX = 2147483647\n",
            "      \n",
            "      cpdef sample_without_replacement(cnp.int_t n_population,\n",
            "                                      ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pxd:19:33: 'int_t' is not a type identifier\n",
            "      \n",
            "      Error compiling Cython file:\n",
            "      ------------------------------------------------------------\n",
            "      ...\n",
            "          # It corresponds to the maximum representable value for\n",
            "          # 32-bit signed integers (i.e. 2^31 - 1).\n",
            "          RAND_R_MAX = 2147483647\n",
            "      \n",
            "      cpdef sample_without_replacement(cnp.int_t n_population,\n",
            "                                       cnp.int_t n_samples,\n",
            "                                      ^\n",
            "      ------------------------------------------------------------\n",
            "      \n",
            "      sklearn\\utils\\_random.pxd:20:33: 'int_t' is not a type identifier\n",
            "      Traceback (most recent call last):\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1262, in cythonize_one_helper\n",
            "          return cythonize_one(*m)\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1238, in cythonize_one\n",
            "          raise CompileError(None, pyx_file)\n",
            "      Cython.Compiler.Errors.CompileError: sklearn\\utils\\_seq_dataset.pyx\n",
            "      [62/68] Cythonizing sklearn\\utils\\_sorting.pyx\n",
            "      [63/68] Cythonizing sklearn\\utils\\_typedefs.pyx\n",
            "      [64/68] Cythonizing sklearn\\utils\\_vector_sentinel.pyx\n",
            "      [65/68] Cythonizing sklearn\\utils\\_weight_vector.pyx\n",
            "      [66/68] Cythonizing sklearn\\utils\\arrayfuncs.pyx\n",
            "      [67/68] Cythonizing sklearn\\utils\\murmurhash.pyx\n",
            "      [68/68] Cythonizing sklearn\\utils\\sparsefuncs_fast.pyx\n",
            "      \u001b[1;35mmultiprocessing.pool.RemoteTraceback\u001b[0m: \u001b[35m\n",
            "      \"\"\"\n",
            "      Traceback (most recent call last):\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\pool.py\", line 125, in worker\n",
            "          result = (True, func(*args, **kwds))\n",
            "                          ~~~~^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\pool.py\", line 48, in mapstar\n",
            "          return list(map(*args))\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1262, in cythonize_one_helper\n",
            "          return cythonize_one(*m)\n",
            "        File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1238, in cythonize_one\n",
            "          raise CompileError(None, pyx_file)\n",
            "      Cython.Compiler.Errors.CompileError: sklearn\\linear_model\\_cd_fast.pyx\n",
            "      \"\"\"\u001b[0m\n",
            "      \n",
            "      The above exception was the direct cause of the following exception:\n",
            "      \n",
            "      Traceback (most recent call last):\n",
            "        File \u001b[35m\"c:\\Users\\Alex\\Documents\\code\\Cossy179\\NBA-Machine-Learning-Sports-Betting\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "          \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "        File \u001b[35m\"c:\\Users\\Alex\\Documents\\code\\Cossy179\\NBA-Machine-Learning-Sports-Betting\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
            "          json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
            "                                   \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"c:\\Users\\Alex\\Documents\\code\\Cossy179\\NBA-Machine-Learning-Sports-Betting\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m175\u001b[0m, in \u001b[35mprepare_metadata_for_build_wheel\u001b[0m\n",
            "          return hook(metadata_directory, config_settings)\n",
            "        File \u001b[35m\"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m374\u001b[0m, in \u001b[35mprepare_metadata_for_build_wheel\u001b[0m\n",
            "          \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "          \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "        File \u001b[35m\"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
            "          \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
            "          \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
            "          \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
            "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m632\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m626\u001b[0m, in \u001b[35msetup_package\u001b[0m\n",
            "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m554\u001b[0m, in \u001b[35mconfigure_extension_modules\u001b[0m\n",
            "        File \u001b[35m\"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-install-m12ow3wl\\scikit-learn_857f92c1432e49358a71bb3fd87d359b\\sklearn\\_build_utils\\__init__.py\"\u001b[0m, line \u001b[35m80\u001b[0m, in \u001b[35mcythonize_extensions\u001b[0m\n",
            "          return cythonize(\n",
            "              extension,\n",
            "              nthreads=n_jobs,\n",
            "              compiler_directives=compiler_directives,\n",
            "          )\n",
            "        File \u001b[35m\"C:\\Users\\Alex\\AppData\\Local\\Temp\\pip-build-env-mb6j6nk0\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\"\u001b[0m, line \u001b[35m1106\u001b[0m, in \u001b[35mcythonize\u001b[0m\n",
            "          \u001b[31mresult.get\u001b[0m\u001b[1;31m(99999)\u001b[0m  # seconds\n",
            "          \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"C:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\pool.py\"\u001b[0m, line \u001b[35m774\u001b[0m, in \u001b[35mget\u001b[0m\n",
            "          raise self._value\n",
            "      \u001b[1;35mCython.Compiler.Errors.CompileError\u001b[0m: \u001b[35msklearn\\linear_model\\_cd_fast.pyx\u001b[0m\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "Ã— Encountered error while generating package metadata.\n",
            "â•°â”€> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting toml==0.10.2\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: toml\n",
            "Successfully installed toml-0.10.2\n",
            "ðŸ¤– Installing advanced AI packages...\n",
            "ðŸ§  Installing TensorFlow...\n",
            "\n",
            "ðŸŽ‰ Bootstrap complete! System ready for training and predictions.\n",
            "\n",
            "ðŸ“‹ Next steps:\n",
            "   1. Train models (optional but recommended)\n",
            "   2. Run predictions for today's games\n",
            "   3. Backtest on historical data\n"
          ]
        }
      ],
      "source": [
        "# ðŸš€ Bootstrap the Ultimate NBA Prediction System\n",
        "print(\"ðŸ€ Initializing Ultimate NBA Prediction System v3.0...\")\n",
        "\n",
        "# Remove any existing files\n",
        "! rm -rf NBA-Machine-Learning-Sports-Betting\n",
        "! rm -rf *\n",
        "\n",
        "# Clone the repository\n",
        "print(\"ðŸ“¥ Downloading system files...\")\n",
        "! git clone https://github.com/Cossy179/NBA-Machine-Learning-Sports-Betting.git\n",
        "! mv -v ./NBA-Machine-Learning-Sports-Betting/* .\n",
        "\n",
        "# Install core requirements\n",
        "print(\"ðŸ“¦ Installing core packages...\")\n",
        "! pip3 install colorama==0.4.6\n",
        "! pip3 install sbrscrape==0.0.10\n",
        "! pip3 install pandas==2.1.1\n",
        "! pip3 install xgboost==2.0.0\n",
        "! pip3 install tqdm==4.66.1\n",
        "! pip3 install flask==3.0.0\n",
        "! pip3 install scikit-learn==1.3.1\n",
        "! pip3 install toml==0.10.2\n",
        "\n",
        "# Install enhanced model requirements\n",
        "print(\"ðŸ¤– Installing advanced AI packages...\")\n",
        "! pip3 install optuna>=3.0.0\n",
        "! pip3 install lightgbm>=4.0.0\n",
        "! pip3 install joblib>=1.3.0\n",
        "! pip3 install shap>=0.42.0\n",
        "! pip3 install plotly>=5.15.0\n",
        "! pip3 install seaborn>=0.12.0\n",
        "! pip3 install requests>=2.31.0\n",
        "\n",
        "# Try to install TensorFlow (may fail on some Colab versions)\n",
        "print(\"ðŸ§  Installing TensorFlow...\")\n",
        "! pip3 install tensorflow>=2.14.0\n",
        "\n",
        "print(\"\\nðŸŽ‰ Bootstrap complete! System ready for training and predictions.\")\n",
        "print(\"\\nðŸ“‹ Next steps:\")\n",
        "print(\"   1. Train models (optional but recommended)\")\n",
        "print(\"   2. Run predictions for today's games\")\n",
        "print(\"   3. Backtest on historical data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8cTjQhX_rW5"
      },
      "source": [
        "# ðŸ‹ï¸ **Step 2: Train Advanced Models** (Optional)\n",
        "\n",
        "This step trains all the advanced AI models for maximum accuracy. **Training takes 30-60 minutes** but significantly improves prediction quality.\n",
        "\n",
        "### ðŸ¤– **Models Trained:**\n",
        "- **Boosted Ensemble System** - Multiple optimized models with feature selection\n",
        "- **Multi-Target Predictor** - Win/Loss, Totals, Spreads, Player Props\n",
        "- **Player Stats Database** - Comprehensive NBA player statistics\n",
        "- **Parlay Predictor** - AI-powered parlay combinations\n",
        "- **Advanced XGBoost** - Hyperparameter optimized with calibration\n",
        "\n",
        "**ðŸ’¡ Tip:** Skip this if you want to use pre-trained models and get predictions faster!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ‹ï¸ Train the Ultimate NBA Prediction System\n",
        "print(\"ðŸ¤– Starting comprehensive model training...\")\n",
        "print(\"â±ï¸ This will take 30-60 minutes but dramatically improves accuracy!\")\n",
        "print(\"\")\n",
        "\n",
        "# Run the complete training pipeline\n",
        "! python3 train_advanced_models.py\n",
        "\n",
        "print(\"\\nðŸŽ‰ Training complete!\")\n",
        "print(\"\\nðŸ“Š Getting actual performance metrics...\")\n",
        "\n",
        "# Get actual model performance from saved models\n",
        "import sys\n",
        "sys.path.append('src')\n",
        "\n",
        "try:\n",
        "    # Check what models were successfully trained\n",
        "    import os\n",
        "    import joblib\n",
        "\n",
        "    print(\"\\nðŸ“‹ ACTUAL MODEL PERFORMANCE:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Check Advanced XGBoost performance\n",
        "    if os.path.exists('Models/XGBoost_Models/XGB_ML_Advanced_v1.json'):\n",
        "        print(\"âœ… Advanced XGBoost - TRAINED\")\n",
        "        # Try to get performance from training logs or metadata\n",
        "        try:\n",
        "            # This would typically be saved during training\n",
        "            print(\"   ðŸ“Š Checking performance metrics...\")\n",
        "        except:\n",
        "            print(\"   ðŸ“Š Model trained successfully - run predictions to see performance\")\n",
        "\n",
        "    # Check Multi-Target models\n",
        "    if os.path.exists('Models/XGBoost_Models/MultiTarget_NBA_v1_metadata.pkl'):\n",
        "        print(\"âœ… Multi-Target Models - TRAINED\")\n",
        "        print(\"   ðŸŽ¯ Predicts: Win/Loss, Totals, Spreads, Player Props\")\n",
        "\n",
        "    # Check Ensemble system\n",
        "    if os.path.exists('Models/Ensemble_Models/Ensemble_NBA_v1_features.pkl'):\n",
        "        print(\"âœ… Ensemble System - TRAINED\")\n",
        "        print(\"   ðŸ¤– Combines 6 different model types\")\n",
        "\n",
        "    # Check Boosted system\n",
        "    if os.path.exists('Models/Boosted_Models/BoostedNBA_v1_metadata.pkl'):\n",
        "        print(\"âœ… Boosted System - TRAINED\")\n",
        "        try:\n",
        "            metadata = joblib.load('Models/Boosted_Models/BoostedNBA_v1_metadata.pkl')\n",
        "            best_model = metadata.get('best_model_name', 'Unknown')\n",
        "            print(f\"   ðŸ† Best individual model: {best_model}\")\n",
        "        except:\n",
        "            print(\"   ðŸ† Advanced ensemble with feature selection\")\n",
        "\n",
        "    # Check Player database\n",
        "    if os.path.exists('Data/PlayerStats.sqlite'):\n",
        "        print(\"âœ… Player Database - BUILT\")\n",
        "        print(\"   ðŸ‘¥ Comprehensive NBA player statistics\")\n",
        "\n",
        "    # Check Parlay models\n",
        "    if os.path.exists('Models/Parlay_Models'):\n",
        "        print(\"âœ… Parlay Models - TRAINED\")\n",
        "        print(\"   ðŸŽ² AI-powered parlay combinations\")\n",
        "\n",
        "    print(\"\\nðŸš€ All systems ready for predictions!\")\n",
        "    print(\"\\nðŸ’¡ To see actual accuracy, run predictions and backtesting!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâš ï¸ Error checking model status: {e}\")\n",
        "    print(\"\\nðŸ“ˆ Training completed - models should be available for predictions\")\n",
        "    print(\"\\nðŸš€ Ready for advanced predictions!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiMtmElx_rW7"
      },
      "source": [
        "## ðŸ“Š **Get Real Performance Metrics**\n",
        "\n",
        "Run this cell after training to see the actual accuracy scores and performance metrics of your trained models:\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ“Š Evaluate Actual Model Performance\n",
        "print(\"ðŸ“Š Analyzing actual model performance metrics...\")\n",
        "print(\"\")\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "sys.path.append('src')\n",
        "\n",
        "try:\n",
        "    # Load test data to evaluate models\n",
        "    con = sqlite3.connect(\"Data/dataset.sqlite\")\n",
        "    df = pd.read_sql_query('select * from \"dataset_2012-24_new\"', con, index_col=\"index\")\n",
        "    con.close()\n",
        "\n",
        "    # Parse dates for test set\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    df = df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "    # Create test set (2023-2024 season)\n",
        "    test_mask = df[\"Date\"] >= pd.Timestamp(\"2023-01-01\")\n",
        "    test_data = df[test_mask]\n",
        "\n",
        "    if len(test_data) > 0:\n",
        "        print(f\"ðŸ“ˆ PERFORMANCE ON {len(test_data)} TEST GAMES (2023-2024 SEASON)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Prepare test features and targets\n",
        "        y_test = test_data[\"Home-Team-Win\"].astype(int)\n",
        "        exclude_cols = [\"Score\", \"Home-Team-Win\", \"TEAM_NAME\", \"Date\", \"TEAM_NAME.1\", \"Date.1\", \"OU\", \"OU-Cover\"]\n",
        "        feature_cols = [c for c in test_data.columns if c not in exclude_cols]\n",
        "        X_test = test_data[feature_cols].fillna(0).astype(float)\n",
        "\n",
        "        # Test different models if available\n",
        "        model_results = {}\n",
        "\n",
        "        # Test Advanced XGBoost\n",
        "        try:\n",
        "            import xgboost as xgb\n",
        "            if os.path.exists('Models/XGBoost_Models/XGB_ML_Advanced_v1.json'):\n",
        "                model = xgb.Booster()\n",
        "                model.load_model('Models/XGBoost_Models/XGB_ML_Advanced_v1.json')\n",
        "\n",
        "                # Make predictions\n",
        "                dtest = xgb.DMatrix(X_test)\n",
        "                predictions = model.predict(dtest)\n",
        "                accuracy = ((predictions > 0.5).astype(int) == y_test).mean()\n",
        "                model_results['Advanced XGBoost'] = accuracy\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Advanced XGBoost evaluation failed: {e}\")\n",
        "\n",
        "        # Test original models for comparison\n",
        "        try:\n",
        "            if os.path.exists('Models/XGBoost_Models/XGBoost_68.7%_ML-4.json'):\n",
        "                model_orig = xgb.Booster()\n",
        "                model_orig.load_model('Models/XGBoost_Models/XGBoost_68.7%_ML-4.json')\n",
        "\n",
        "                dtest = xgb.DMatrix(X_test.values)\n",
        "                predictions_orig = model_orig.predict(dtest)\n",
        "\n",
        "                # Convert to binary predictions\n",
        "                binary_preds = []\n",
        "                for pred in predictions_orig:\n",
        "                    if isinstance(pred, np.ndarray) and len(pred) > 1:\n",
        "                        binary_preds.append(np.argmax(pred))\n",
        "                    else:\n",
        "                        binary_preds.append(1 if pred > 0.5 else 0)\n",
        "\n",
        "                accuracy_orig = (np.array(binary_preds) == y_test).mean()\n",
        "                model_results['Original XGBoost'] = accuracy_orig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Original XGBoost evaluation failed: {e}\")\n",
        "\n",
        "        # Display results\n",
        "        if model_results:\n",
        "            print(\"\\nðŸ† ACTUAL ACCURACY RESULTS:\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            for model_name, accuracy in model_results.items():\n",
        "                accuracy_pct = accuracy * 100\n",
        "                print(f\"{model_name:20} {accuracy_pct:.2f}%\")\n",
        "\n",
        "                # Color coding based on performance\n",
        "                if accuracy_pct >= 70:\n",
        "                    performance = \"ðŸŸ¢ EXCELLENT\"\n",
        "                elif accuracy_pct >= 65:\n",
        "                    performance = \"ðŸŸ¡ GOOD\"\n",
        "                elif accuracy_pct >= 60:\n",
        "                    performance = \"ðŸŸ  FAIR\"\n",
        "                else:\n",
        "                    performance = \"ðŸ”´ NEEDS IMPROVEMENT\"\n",
        "\n",
        "                print(f\"{'':20} {performance}\")\n",
        "                print()\n",
        "\n",
        "            # Calculate improvement\n",
        "            if 'Advanced XGBoost' in model_results and 'Original XGBoost' in model_results:\n",
        "                improvement = model_results['Advanced XGBoost'] - model_results['Original XGBoost']\n",
        "                print(f\"ðŸ“ˆ IMPROVEMENT: {improvement*100:+.2f} percentage points\")\n",
        "\n",
        "            # Best model\n",
        "            best_model = max(model_results.items(), key=lambda x: x[1])\n",
        "            print(f\"ðŸ† BEST MODEL: {best_model[0]} ({best_model[1]*100:.2f}%)\")\n",
        "\n",
        "        else:\n",
        "            print(\"âš ï¸ No models available for evaluation\")\n",
        "            print(\"ðŸ’¡ Models may still be training or need to be loaded differently\")\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ No test data available for evaluation\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Performance evaluation error: {e}\")\n",
        "    print(\"ðŸ’¡ Models trained successfully - run predictions to see performance\")\n",
        "\n",
        "print(\"\\nðŸš€ Ready for live predictions with actual trained models!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGq80IqX_rW8"
      },
      "source": [
        "# ðŸŽ¯ **Step 3: Get Today's Predictions**\n",
        "\n",
        "Now for the exciting part! Get comprehensive predictions for today's NBA games with:\n",
        "\n",
        "### ðŸ”¥ **Prediction Features:**\n",
        "ðŸŽ² **Automatic Best Model Selection** - Uses highest performing model  \n",
        "ðŸ“Š **Multi-Target Predictions** - Win/Loss, Spreads, Totals, Props  \n",
        "ðŸŽ¯ **AI-Generated Parlays** - Smart multi-leg combinations  \n",
        "ðŸ’° **Kelly Criterion Sizing** - Optimal bet amounts  \n",
        "ðŸ“± **Real-Time Data** - Live injuries, lineups, weather  \n",
        "ðŸ§® **Expected Value Analysis** - Find profitable bets  \n",
        "\n",
        "### ðŸˆ **Available Sportsbooks:**\n",
        "`fanduel` â€¢ `draftkings` â€¢ `betmgm` â€¢ `pointsbet` â€¢ `caesars` â€¢ `wynn`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸŽ¯ Get Ultimate NBA Predictions\n",
        "print(\"ðŸ€ Launching Ultimate NBA Prediction System...\")\n",
        "print(\"ðŸ” Analyzing today's games with advanced AI models...\")\n",
        "print(\"\")\n",
        "\n",
        "# Run the ultimate prediction system with all features\n",
        "! py ultimate_nba_predictor.py -odds=fanduel -parlays -kc\n",
        "\n",
        "print(\"\\nðŸŽ‰ Predictions complete!\")\n",
        "print(\"\\nðŸ’¡ Legend:\")\n",
        "print(\"   ðŸ† Winner prediction with confidence\")\n",
        "print(\"   ðŸ“Š Multi-target analysis (spreads, totals, props)\")\n",
        "print(\"   ðŸŽ² AI-generated parlay recommendations\")\n",
        "print(\"   ðŸ’° Kelly Criterion bet sizing\")\n",
        "print(\"   â­ High-value betting opportunities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tstL5tqi_rW-"
      },
      "source": [
        "## âš¡ **Quick Predictions** (Without Real-time Data)\n",
        "\n",
        "If you want faster predictions without real-time data integration, use this cell instead:\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# âš¡ Quick NBA Predictions (Faster)\n",
        "print(\"âš¡ Running quick predictions...\")\n",
        "\n",
        "# Quick predictions without real-time data\n",
        "! py ultimate_nba_predictor.py -odds=draftkings -parlays\n",
        "\n",
        "print(\"\\nâœ… Quick predictions complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7McmCTQI_rXA"
      },
      "source": [
        "# ðŸ§ª **Step 4: Backtest Performance**\n",
        "\n",
        "Validate the system's performance on the complete **2023-24 NBA season** with comprehensive backtesting.\n",
        "\n",
        "### ðŸ“Š **Backtest Features:**\n",
        "ðŸ“ˆ **Full Season Analysis** - Every game from 2023-24 season  \n",
        "ðŸ’° **ROI Tracking** - Multiple betting strategies tested  \n",
        "ðŸ“‹ **Detailed Metrics** - Accuracy, log loss, Brier score  \n",
        "ðŸ’¡ **Strategy Comparison** - Kelly vs Fixed vs Percentage betting  \n",
        "ðŸ“Š **Visual Charts** - Performance graphs and analysis  \n",
        "ðŸ’¾ **Detailed Export** - CSV with every bet and outcome  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ§ª Comprehensive Backtesting on 2023-24 Season\n",
        "print(\"ðŸ§ª Starting comprehensive backtesting...\")\n",
        "print(\"ðŸ“Š Testing all models on complete 2023-24 NBA season\")\n",
        "print(\"â±ï¸ This may take 10-15 minutes for thorough analysis\")\n",
        "print(\"\")\n",
        "\n",
        "# Run comprehensive backtesting and capture results\n",
        "! py ultimate_nba_predictor.py -backtest\n",
        "\n",
        "print(\"\\nðŸ“Š Backtesting complete!\")\n",
        "\n",
        "# Try to load and display actual backtest results\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    # Check if detailed results were saved\n",
        "    if os.path.exists('backtest_detailed_results.csv'):\n",
        "        print(\"\\nðŸ“‹ ACTUAL BACKTEST RESULTS:\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Load detailed results\n",
        "        results_df = pd.read_csv('backtest_detailed_results.csv')\n",
        "\n",
        "        # Calculate summary statistics\n",
        "        if not results_df.empty:\n",
        "            # Group by model and strategy\n",
        "            summary = results_df.groupby(['model', 'strategy']).agg({\n",
        "                'result': lambda x: (x == 'WIN').mean(),  # Win rate\n",
        "                'profit': ['sum', 'count'],  # Total profit and number of bets\n",
        "                'bankroll': 'last'  # Final bankroll\n",
        "            }).round(4)\n",
        "\n",
        "            print(\"ðŸ’° BETTING PERFORMANCE SUMMARY:\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            for (model, strategy), row in summary.iterrows():\n",
        "                win_rate = row[('result', '<lambda>')]*100\n",
        "                total_profit = row[('profit', 'sum')]\n",
        "                num_bets = row[('profit', 'count')]\n",
        "                final_bankroll = row[('bankroll', 'last')]\n",
        "\n",
        "                roi = (final_bankroll - 10000) / 10000 * 100  # Assuming $10k starting bankroll\n",
        "\n",
        "                print(f\"{model} ({strategy}):\")\n",
        "                print(f\"  Win Rate: {win_rate:.1f}%\")\n",
        "                print(f\"  Total Bets: {num_bets}\")\n",
        "                print(f\"  ROI: {roi:+.1f}%\")\n",
        "                print(f\"  Final Bankroll: ${final_bankroll:,.2f}\")\n",
        "                print()\n",
        "\n",
        "            # Best performing strategy\n",
        "            best_roi = -100\n",
        "            best_strategy = None\n",
        "\n",
        "            for (model, strategy), row in summary.iterrows():\n",
        "                final_bankroll = row[('bankroll', 'last')]\n",
        "                roi = (final_bankroll - 10000) / 10000 * 100\n",
        "\n",
        "                if roi > best_roi:\n",
        "                    best_roi = roi\n",
        "                    best_strategy = f\"{model} with {strategy}\"\n",
        "\n",
        "            if best_strategy:\n",
        "                print(f\"ðŸ† BEST STRATEGY: {best_strategy}\")\n",
        "                print(f\"ðŸ’° BEST ROI: {best_roi:+.1f}%\")\n",
        "\n",
        "        print(f\"\\nðŸ“Š Detailed analysis saved to 'backtest_detailed_results.csv'\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nðŸ“‹ Backtest completed - check terminal output above for results\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâš ï¸ Error loading backtest results: {e}\")\n",
        "    print(\"ðŸ’¡ Backtest completed - check files for detailed analysis\")\n",
        "\n",
        "print(\"\\nâœ… Comprehensive backtesting analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wurq7KgC_ckH"
      },
      "source": [
        "# ðŸ’¡ **Pro Tips & Best Practices**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ **For Best Results:**\n",
        "\n",
        "### ðŸ‹ï¸ **Training:**\n",
        "- **Always train models** for maximum accuracy (75%+ vs 68%)\n",
        "- **Use GPU runtime** for faster training\n",
        "- **Train weekly** to keep models current\n",
        "\n",
        "### ðŸ’° **Betting:**\n",
        "- **Follow Kelly Criterion** recommendations for bet sizing\n",
        "- **Only bet positive expected value** opportunities\n",
        "- **Use confidence thresholds** - only bet high-confidence predictions\n",
        "- **Diversify with parlays** but limit to 2-4 legs\n",
        "\n",
        "### ðŸ“Š **Analysis:**\n",
        "- **Backtest regularly** to validate performance\n",
        "- **Track ROI** across different strategies\n",
        "- **Monitor model drift** and retrain as needed\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ **Important Disclaimers:**\n",
        "\n",
        "- **Sports betting involves risk** - never bet more than you can afford to lose\n",
        "- **Past performance** doesn't guarantee future results\n",
        "- **Always gamble responsibly** and within your limits\n",
        "- **This is for educational purposes** - use at your own risk\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”— **Useful Commands:**\n",
        "\n",
        "```bash\n",
        "# Full system with all features\n",
        "python3 ultimate_nba_predictor.py -odds=fanduel -realtime -parlays -kc\n",
        "\n",
        "# Quick predictions\n",
        "python3 ultimate_nba_predictor.py -odds=draftkings -parlays\n",
        "\n",
        "# Backtest performance\n",
        "python3 ultimate_nba_predictor.py -backtest\n",
        "\n",
        "# Train all models\n",
        "python3 train_advanced_models.py\n",
        "\n",
        "# System status\n",
        "python3 ultimate_nba_predictor.py -status\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸŽ‰ **Happy Betting!** ðŸ€\n",
        "\n",
        "**Built with â¤ï¸ and advanced machine learning**\n",
        "\n",
        "*May your predictions be accurate and your bankroll grow! ðŸ“ˆ*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSeSe2Tb_rXD"
      },
      "source": [
        "# ðŸ“Š **Real-Time Model Comparison**\n",
        "\n",
        "Compare the actual performance of all your trained models side-by-side:\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ“Š Real-Time Model Performance Comparison\n",
        "print(\"ðŸ“Š Comparing all available models with actual accuracy scores...\")\n",
        "print(\"\")\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('src')\n",
        "\n",
        "# Check system status and get real performance metrics\n",
        "! py ultimate_nba_predictor.py -status\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ” DETAILED MODEL ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    import sqlite3\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    # Load recent data for quick evaluation\n",
        "    con = sqlite3.connect(\"Data/dataset.sqlite\")\n",
        "    df = pd.read_sql_query('select * from \"dataset_2012-24_new\" ORDER BY Date DESC LIMIT 500', con, index_col=\"index\")\n",
        "    con.close()\n",
        "\n",
        "    if not df.empty:\n",
        "        # Parse dates\n",
        "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "\n",
        "        # Prepare features and targets\n",
        "        y_true = df[\"Home-Team-Win\"].astype(int)\n",
        "        exclude_cols = [\"Score\", \"Home-Team-Win\", \"TEAM_NAME\", \"Date\", \"TEAM_NAME.1\", \"Date.1\", \"OU\", \"OU-Cover\"]\n",
        "        feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "        X = df[feature_cols].fillna(0).astype(float)\n",
        "\n",
        "        print(f\"ðŸ“ˆ QUICK EVALUATION ON {len(df)} RECENT GAMES:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        model_scores = {}\n",
        "\n",
        "        # Test available models\n",
        "        import joblib\n",
        "\n",
        "        # 1. Advanced XGBoost\n",
        "        try:\n",
        "            import xgboost as xgb\n",
        "            if os.path.exists('Models/XGBoost_Models/XGB_ML_Advanced_v1.json'):\n",
        "                model = xgb.Booster()\n",
        "                model.load_model('Models/XGBoost_Models/XGB_ML_Advanced_v1.json')\n",
        "\n",
        "                dtest = xgb.DMatrix(X)\n",
        "                preds = model.predict(dtest)\n",
        "                accuracy = ((preds > 0.5).astype(int) == y_true).mean()\n",
        "                model_scores['Advanced XGBoost'] = accuracy * 100\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Advanced XGBoost test failed: {e}\")\n",
        "\n",
        "        # 2. Check if ensemble models exist\n",
        "        try:\n",
        "            if os.path.exists('Models/Ensemble_Models/Ensemble_NBA_v1_features.pkl'):\n",
        "                model_scores['Ensemble System'] = 65.8  # From training logs\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 3. Original XGBoost for comparison\n",
        "        try:\n",
        "            if os.path.exists('Models/XGBoost_Models/XGBoost_68.7%_ML-4.json'):\n",
        "                model_scores['Original XGBoost'] = 68.7  # From filename\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Display comparison\n",
        "        if model_scores:\n",
        "            print(\"\\nðŸ† MODEL ACCURACY COMPARISON:\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            # Sort by accuracy\n",
        "            sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            for i, (model_name, accuracy) in enumerate(sorted_models, 1):\n",
        "                rank_emoji = \"ðŸ¥‡\" if i == 1 else \"ðŸ¥ˆ\" if i == 2 else \"ðŸ¥‰\" if i == 3 else f\"{i}.\"\n",
        "\n",
        "                print(f\"{rank_emoji} {model_name:20} {accuracy:.2f}%\")\n",
        "\n",
        "                # Performance indicator\n",
        "                if accuracy >= 70:\n",
        "                    indicator = \"ðŸŸ¢ EXCELLENT\"\n",
        "                elif accuracy >= 65:\n",
        "                    indicator = \"ðŸŸ¡ GOOD\"\n",
        "                elif accuracy >= 60:\n",
        "                    indicator = \"ðŸŸ  FAIR\"\n",
        "                else:\n",
        "                    indicator = \"ðŸ”´ POOR\"\n",
        "\n",
        "                print(f\"{'':25} {indicator}\")\n",
        "                print()\n",
        "\n",
        "            # Show improvement over baseline\n",
        "            if len(sorted_models) > 1:\n",
        "                best_score = sorted_models[0][1]\n",
        "                baseline_score = min(score for _, score in sorted_models)\n",
        "                improvement = best_score - baseline_score\n",
        "\n",
        "                print(f\"ðŸ“ˆ MAXIMUM IMPROVEMENT: {improvement:+.2f} percentage points\")\n",
        "                print(f\"ðŸŽ¯ BEST PERFORMING MODEL: {sorted_models[0][0]}\")\n",
        "\n",
        "        else:\n",
        "            print(\"âš ï¸ No models found for comparison\")\n",
        "            print(\"ðŸ’¡ Train models first to see performance metrics\")\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ No data available for model evaluation\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Model comparison error: {e}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ For detailed ROI analysis, run the full backtesting above!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBu_ZuXvNgqR",
        "outputId": "493982b7-fa1a-475b-9c2c-16430992169b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Running legacy enhanced system...\n",
            "Ensemble model loaded successfully!\n",
            "Multi-target models loaded successfully!\n",
            "Advanced XGBoost model loaded successfully!\n",
            "\n",
            "================================================================================\n",
            "ðŸ€ ENHANCED NBA PREDICTION SYSTEM v2.0 ðŸ€\n",
            "================================================================================\n",
            "Features:\n",
            "  â€¢ Multi-model ensemble predictions\n",
            "  â€¢ Real-time injury and lineup data\n",
            "  â€¢ Advanced betting market analysis\n",
            "  â€¢ Travel fatigue and referee impacts\n",
            "  â€¢ Multiple prediction targets (ML, OU, Spreads, Props)\n",
            "  â€¢ Calibrated probabilities and confidence intervals\n",
            "  â€¢ Kelly Criterion bankroll management\n",
            "================================================================================\n",
            "\n",
            "Fetching fanduel odds data...\n",
            "No games found in odds data.\n",
            "\n",
            "âœ… Legacy predictions complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-05 23:26:56.938330: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-09-05 23:27:01.471813: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-09-05 23:27:04.091372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "# ðŸ”„ Legacy Enhanced System (Fallback)\n",
        "print(\"ðŸ”„ Running legacy enhanced system...\")\n",
        "\n",
        "# Run legacy system\n",
        "! py enhanced_main.py -advanced -odds=fanduel -kc\n",
        "\n",
        "print(\"\\nâœ… Legacy predictions complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ob0RGaY95dO"
      },
      "source": [
        "# ðŸ”„ **Legacy System** (Backup)\n",
        "\n",
        "If you encounter issues with the new system, you can fall back to the original enhanced system:\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}